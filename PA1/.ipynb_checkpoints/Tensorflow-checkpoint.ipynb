{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Importing Libraries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./train-images-idx3-ubyte.gz\n",
      "Extracting ./train-labels-idx1-ubyte.gz\n",
      "Extracting ./t10k-images-idx3-ubyte.gz\n",
      "Extracting ./t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#Read Data\n",
    "data = input_data.read_data_sets(\".\",one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weight(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bias(shape):\n",
    "    initial = tf.constant(0.0, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_layer():\n",
    "    return [200,100,10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem4 FC and RELu only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "#correct labels\n",
    "y_labels = tf.placeholder(tf.float32, [None, 10])\n",
    "#y = tf.placeholder(tf.int64, [None])\n",
    "#get layer\n",
    "layer1,layer2,layer3 = get_layer()\n",
    "#init weight and bias\n",
    "x_image = tf.reshape(x,[-1,784])\n",
    "W1 = get_weight([784,layer1])\n",
    "b1 = get_bias([layer1])\n",
    "\n",
    "W2 = get_weight([layer1,layer2])\n",
    "b2 = get_bias([layer2])\n",
    "\n",
    "W3 = get_weight([layer2,layer3])\n",
    "b3 = get_bias([layer3])\n",
    "\n",
    "#define model\n",
    "y1 = tf.nn.relu(tf.matmul(x_image, W1) + b1)\n",
    "\n",
    "\n",
    "y2 = tf.nn.relu(tf.matmul(y1, W2) + b2)\n",
    "\n",
    "y3 = tf.matmul(y2, W3) + b3\n",
    "output = tf.nn.softmax(y3)\n",
    "output_class = tf.argmax(output,axis=1)\n",
    "\n",
    "#Define loss function\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = y3, labels = y_labels))\n",
    "\n",
    "#Accuracy\n",
    "correct_labels = tf.equal(tf.argmax(y3, 1), tf.argmax(y_labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_labels, tf.float32))\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "# create a saver\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# initialize the graph\n",
    "init = tf.global_variables_initializer()\n",
    "#sess = tf.Session()\n",
    "#sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_fc_train = []\n",
    "loss_fc_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "2.30254\n",
      "2.27826\n",
      "2.09666\n",
      "1.70505\n",
      "1.31329\n",
      "1.05335\n",
      "0.905689\n",
      "0.813201\n",
      "0.754792\n",
      "0.706359\n",
      "0.675927\n",
      "0.647649\n",
      "0.62706\n",
      "0.604237\n",
      "0.579158\n",
      "0.561745\n",
      "0.544085\n",
      "0.529419\n",
      "0.51551\n",
      "0.502479\n",
      "0.488785\n",
      "0.47672\n",
      "0.465009\n",
      "0.457617\n",
      "0.447389\n",
      "0.439739\n",
      "0.43582\n",
      "0.426847\n",
      "0.425759\n",
      "0.418983\n",
      "0.409688\n",
      "0.404908\n",
      "0.404852\n",
      "0.399072\n",
      "0.398968\n",
      "0.390984\n",
      "0.388089\n",
      "0.384773\n",
      "0.383269\n",
      "0.37538\n",
      "0.373461\n",
      "0.370541\n",
      "0.369967\n",
      "0.364206\n",
      "0.363233\n",
      "0.361913\n",
      "0.35969\n",
      "0.364447\n",
      "0.358879\n",
      "0.355113\n",
      "0.351894\n",
      "0.352784\n",
      "0.349405\n",
      "0.34461\n",
      "0.346616\n",
      "0.343501\n",
      "0.33861\n",
      "0.340865\n",
      "0.338336\n",
      "0.332875\n",
      "0.332674\n",
      "0.335305\n",
      "0.328478\n",
      "0.33123\n",
      "0.327257\n",
      "0.32591\n",
      "0.323413\n",
      "0.322984\n",
      "0.322087\n",
      "0.318525\n",
      "0.31557\n",
      "0.315747\n",
      "0.313501\n",
      "0.310587\n",
      "0.311769\n",
      "0.313177\n",
      "0.313015\n",
      "0.307836\n",
      "0.304227\n",
      "0.305428\n",
      "0.306832\n",
      "0.302683\n",
      "0.302392\n",
      "0.299417\n",
      "0.300211\n",
      "0.295181\n",
      "0.294972\n",
      "0.299993\n",
      "0.29434\n",
      "0.291726\n",
      "0.293055\n",
      "0.288937\n",
      "0.289711\n",
      "0.290602\n",
      "0.283132\n",
      "0.285543\n",
      "0.286708\n",
      "0.284715\n",
      "0.278192\n",
      "0.284783\n",
      "0.278277\n",
      "0.277443\n",
      "0.272644\n",
      "0.275766\n",
      "0.270425\n",
      "0.269678\n",
      "0.269422\n",
      "0.267424\n",
      "0.270025\n",
      "0.263405\n",
      "0.262659\n",
      "0.263015\n",
      "0.263082\n",
      "0.262234\n",
      "0.264014\n",
      "0.261894\n",
      "0.259304\n",
      "0.259325\n",
      "0.255345\n",
      "0.255382\n",
      "0.251478\n",
      "0.253727\n",
      "0.253778\n",
      "0.250887\n",
      "0.246525\n",
      "0.247238\n",
      "0.2446\n",
      "0.244583\n",
      "0.243996\n",
      "0.24225\n",
      "0.242698\n",
      "0.240721\n",
      "0.23997\n",
      "0.242891\n",
      "0.235934\n",
      "0.235739\n",
      "0.238649\n",
      "0.23107\n",
      "0.232767\n",
      "0.236837\n",
      "0.227585\n",
      "0.230028\n",
      "0.228095\n",
      "0.224907\n",
      "0.226373\n",
      "0.224765\n",
      "0.223418\n",
      "0.22634\n",
      "0.222426\n",
      "0.220134\n",
      "0.222674\n",
      "0.21987\n",
      "0.21804\n",
      "0.216619\n",
      "0.216139\n",
      "0.216634\n",
      "0.213121\n",
      "0.209777\n",
      "0.211889\n",
      "0.211618\n",
      "0.209655\n",
      "0.212145\n",
      "0.207313\n",
      "0.203912\n",
      "0.205913\n",
      "0.203835\n",
      "0.20322\n",
      "0.207425\n",
      "0.20545\n",
      "0.204004\n",
      "0.19983\n",
      "0.204222\n",
      "0.200127\n",
      "0.198911\n",
      "0.195824\n",
      "0.197707\n",
      "0.19637\n",
      "0.198797\n",
      "0.197639\n",
      "0.191196\n",
      "0.194985\n",
      "0.193705\n",
      "0.190733\n",
      "0.191399\n",
      "0.190865\n",
      "0.189611\n",
      "0.187901\n",
      "0.189105\n",
      "0.187043\n",
      "0.184594\n",
      "0.186492\n",
      "0.183181\n",
      "0.186514\n",
      "0.184693\n",
      "0.182369\n",
      "0.182236\n",
      "0.184236\n",
      "0.184466\n",
      "0.180738\n",
      "0.183718\n",
      "0.183871\n",
      "0.179906\n",
      "0.183014\n",
      "0.178526\n",
      "0.176118\n",
      "0.178709\n",
      "0.174647\n",
      "0.175951\n",
      "0.175576\n",
      "0.172721\n",
      "0.175518\n",
      "0.173229\n",
      "0.173203\n",
      "0.171825\n",
      "0.170464\n",
      "0.173245\n",
      "0.167212\n",
      "0.168334\n",
      "0.169928\n",
      "0.168897\n",
      "0.166557\n",
      "0.165505\n",
      "0.167014\n",
      "0.166807\n",
      "0.165503\n",
      "0.166313\n",
      "0.168916\n",
      "0.164727\n",
      "0.164754\n",
      "0.163532\n",
      "0.165186\n",
      "0.16282\n",
      "0.164408\n",
      "0.159251\n",
      "0.158381\n",
      "0.160746\n",
      "0.161402\n",
      "0.159227\n",
      "0.157501\n",
      "0.155995\n",
      "0.160011\n",
      "0.162001\n",
      "0.157431\n",
      "0.154174\n",
      "0.15658\n",
      "0.162632\n",
      "0.152771\n",
      "0.153882\n",
      "0.154244\n",
      "0.152375\n",
      "0.1511\n",
      "0.15271\n",
      "0.15115\n",
      "0.151596\n",
      "0.150299\n",
      "0.151963\n",
      "0.148502\n",
      "0.149219\n",
      "0.148124\n",
      "0.149483\n",
      "0.14744\n",
      "0.147621\n",
      "0.148783\n",
      "0.148568\n",
      "0.148461\n",
      "0.143842\n",
      "0.14467\n",
      "0.143927\n",
      "0.147298\n",
      "0.14342\n",
      "0.143306\n",
      "0.1416\n",
      "0.145307\n",
      "0.1425\n",
      "0.142678\n",
      "0.142341\n",
      "0.14105\n",
      "0.141917\n",
      "0.143228\n",
      "0.142805\n",
      "0.140277\n",
      "0.14116\n",
      "0.13978\n",
      "0.14507\n",
      "0.140449\n",
      "0.138402\n",
      "0.139129\n",
      "0.136713\n",
      "0.137462\n",
      "0.141236\n",
      "0.13839\n",
      "0.137349\n",
      "0.134535\n",
      "0.135221\n",
      "0.135385\n",
      "0.133058\n",
      "0.134316\n",
      "0.132924\n",
      "0.134646\n",
      "0.131571\n",
      "0.132519\n",
      "0.131078\n",
      "0.132708\n",
      "0.131142\n",
      "0.134231\n",
      "0.136929\n",
      "0.130875\n",
      "0.131206\n",
      "0.130276\n",
      "0.130754\n",
      "0.129407\n",
      "0.129889\n",
      "0.131426\n",
      "0.133179\n",
      "0.129272\n",
      "0.131589\n",
      "0.129825\n",
      "0.129298\n",
      "0.126896\n",
      "0.126436\n",
      "0.126523\n",
      "0.131037\n",
      "0.126458\n",
      "0.126532\n",
      "0.125759\n",
      "0.126528\n",
      "0.126607\n",
      "0.125989\n",
      "0.129218\n",
      "0.124444\n",
      "0.123669\n",
      "0.126687\n",
      "0.125612\n",
      "0.123052\n",
      "0.123592\n",
      "0.129224\n",
      "0.126143\n",
      "0.12139\n",
      "0.122211\n",
      "0.122917\n",
      "0.120918\n",
      "0.126037\n",
      "0.122256\n",
      "0.121137\n",
      "0.119851\n",
      "0.119688\n",
      "0.124086\n",
      "0.119241\n",
      "0.118803\n",
      "0.120369\n",
      "0.119622\n",
      "0.123135\n",
      "0.119556\n",
      "0.11631\n",
      "0.116604\n",
      "0.116087\n",
      "0.115884\n",
      "0.116368\n",
      "0.119767\n",
      "0.11661\n",
      "0.11687\n",
      "0.118282\n",
      "0.117025\n",
      "0.115811\n",
      "0.118689\n",
      "0.116083\n",
      "0.114274\n",
      "0.119332\n",
      "0.114429\n",
      "0.112879\n",
      "0.113032\n",
      "0.11435\n",
      "0.113451\n",
      "0.112806\n",
      "0.114493\n",
      "0.114621\n",
      "0.111028\n",
      "0.111034\n",
      "0.112936\n",
      "0.110735\n",
      "0.111454\n",
      "0.11074\n",
      "0.11134\n",
      "0.112071\n",
      "0.113668\n",
      "0.112456\n",
      "0.11118\n",
      "0.108656\n",
      "0.110359\n",
      "0.108158\n",
      "0.113056\n",
      "0.112176\n",
      "0.11099\n",
      "0.108892\n",
      "0.108765\n",
      "0.107651\n",
      "0.109503\n",
      "0.108421\n",
      "0.107116\n",
      "0.107099\n",
      "test accuracy 0.9688\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "batch_size = 55\n",
    "print(\"Start\")\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(20000):\n",
    "        batch_img, batch_y = data.train.next_batch(batch_size)\n",
    "        _, l = sess.run([train_step, loss],feed_dict = {x: batch_img , y_labels: batch_y})\n",
    "        loss_fc_train.append(l)\n",
    "                \n",
    "        if i % 50 == 0:\n",
    "            _, ll = sess.run([train_step, loss],feed_dict = {x: data.test.images, y_labels: data.test.labels})\n",
    "            loss_fc_test.append(ll)\n",
    "            print(ll)\n",
    "        \n",
    "    print('test accuracy %g' % accuracy.eval(feed_dict={\n",
    "            x: data.test.images, y_labels: data.test.labels}))\n",
    "#print('test accuracy %g' % accuracy.eval(feed_dict={x: data.test.images, y_labels: data.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqbN+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x212d4b0bd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss(loss):\n",
    "    x = [i for i in range(len(loss))]\n",
    "    plt.plot(x, loss)\n",
    "    plt.show()\n",
    "plot_loss(loss_fc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 4 With Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-3\n",
    "#input\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "#correct labels\n",
    "y_labels = tf.placeholder(tf.float32, [None, 10])\n",
    "#y = tf.placeholder(tf.int64, [None])\n",
    "#get layer\n",
    "layer1,layer2,layer3 = get_layer()\n",
    "#init weight and bias\n",
    "x_image = tf.reshape(x,[-1,784])\n",
    "\n",
    "\n",
    "W1 = get_weight([784,layer1])\n",
    "#b1 = get_bias([layer1])\n",
    "z1_BN = tf.matmul(x_image,W1)\n",
    "batch_mean1, batch_var1 = tf.nn.moments(z1_BN,[0])\n",
    "scale1 = tf.Variable(tf.ones([layer1]))\n",
    "beta1 = tf.Variable(tf.zeros([layer1]))\n",
    "BN1 = tf.nn.batch_normalization(z1_BN,batch_mean1,batch_var1,beta1,scale1,epsilon)\n",
    "l1_BN = tf.nn.sigmoid(BN1)\n",
    "\n",
    "W2 = get_weight([layer1,layer2])\n",
    "z2_BN = tf.matmul(l1_BN,W2)\n",
    "batch_mean2, batch_var2 = tf.nn.moments(z2_BN,[0])\n",
    "scale2 = tf.Variable(tf.ones([layer2]))\n",
    "beta2 = tf.Variable(tf.zeros([layer2]))\n",
    "BN2 = tf.nn.batch_normalization(z2_BN,batch_mean2,batch_var2,beta2,scale2,epsilon)\n",
    "l2_BN = tf.nn.sigmoid(BN2)\n",
    "\n",
    "\n",
    "\n",
    "W3 = get_weight([layer2,layer3])\n",
    "b3 = get_bias([layer3])\n",
    "\n",
    "#define model\n",
    "#y = tf.nn.relu(tf.matmul(x_image, W1) + b1)\n",
    "\n",
    "output = tf.nn.softmax(tf.matmul(l2_BN,W3)+b3)\n",
    "output_class = tf.argmax(output,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss, optimizer and predictions\n",
    "cross_entropy_BN = -tf.reduce_sum(y_labels*tf.log(output))\n",
    "train_step_BN = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy_BN)\n",
    "correct_prediction_BN = tf.equal(tf.argmax(output,1),tf.argmax(y_labels,1))\n",
    "accuracy_BN = tf.reduce_mean(tf.cast(correct_prediction_BN,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 24882.447)\n",
      "(50, 6539.7197)\n",
      "(100, 5010.1152)\n",
      "(150, 4447.875)\n",
      "(200, 3884.6936)\n",
      "(250, 3404.7949)\n",
      "(300, 3477.8687)\n",
      "(350, 3296.5598)\n",
      "(400, 3080.6641)\n",
      "(450, 3048.0161)\n",
      "(500, 3145.136)\n",
      "(550, 2852.8691)\n",
      "(600, 2879.1177)\n",
      "(650, 2828.9529)\n",
      "(700, 2751.7349)\n",
      "(750, 2740.1331)\n",
      "(800, 2917.5112)\n",
      "(850, 2593.354)\n",
      "(900, 2612.4067)\n",
      "(950, 2728.1995)\n",
      "(1000, 2447.854)\n",
      "(1050, 2554.7988)\n",
      "(1100, 2445.8992)\n",
      "(1150, 2465.7112)\n",
      "(1200, 2375.8325)\n",
      "(1250, 2344.5364)\n",
      "(1300, 2278.9346)\n",
      "(1350, 2375.563)\n",
      "(1400, 2314.3167)\n",
      "(1450, 2223.4004)\n",
      "(1500, 2257.6943)\n",
      "(1550, 2098.1589)\n",
      "(1600, 2072.4414)\n",
      "(1650, 2234.5537)\n",
      "(1700, 1967.9061)\n",
      "(1750, 2018.9716)\n",
      "(1800, 1947.9741)\n",
      "(1850, 1997.637)\n",
      "(1900, 1943.4519)\n",
      "(1950, 1804.1733)\n",
      "(2000, 1796.6545)\n",
      "(2050, 1884.093)\n",
      "(2100, 1845.0789)\n",
      "(2150, 1785.9249)\n",
      "(2200, 1810.3086)\n",
      "(2250, 1708.8801)\n",
      "(2300, 1724.0461)\n",
      "(2350, 1713.6232)\n",
      "(2400, 1671.2802)\n",
      "(2450, 1606.9281)\n",
      "(2500, 1637.7986)\n",
      "(2550, 1554.1644)\n",
      "(2600, 1596.2919)\n",
      "(2650, 1476.8054)\n",
      "(2700, 1589.4968)\n",
      "(2750, 1474.6121)\n",
      "(2800, 1397.238)\n",
      "(2850, 1437.9659)\n",
      "(2900, 1465.2268)\n",
      "(2950, 1671.2461)\n",
      "(3000, 1476.7789)\n",
      "(3050, 1427.5281)\n",
      "(3100, 1422.0299)\n",
      "(3150, 1383.2673)\n",
      "(3200, 1370.7229)\n",
      "(3250, 1453.3157)\n",
      "(3300, 1330.3877)\n",
      "(3350, 1333.52)\n",
      "(3400, 1269.9164)\n",
      "(3450, 1235.8025)\n",
      "(3500, 1363.4991)\n",
      "(3550, 1357.2952)\n",
      "(3600, 1243.6536)\n",
      "(3650, 1117.5245)\n",
      "(3700, 1311.0012)\n",
      "(3750, 1197.1885)\n",
      "(3800, 1103.9648)\n",
      "(3850, 1129.6495)\n",
      "(3900, 1166.623)\n",
      "(3950, 1145.2386)\n",
      "(4000, 1220.3096)\n",
      "(4050, 1155.1025)\n",
      "(4100, 1259.1445)\n",
      "(4150, 1137.1646)\n",
      "(4200, 1119.6052)\n",
      "(4250, 1203.6755)\n",
      "(4300, 1125.8965)\n",
      "(4350, 1103.0017)\n",
      "(4400, 1087.6196)\n",
      "(4450, 1102.5782)\n",
      "(4500, 1099.7513)\n",
      "(4550, 1111.6654)\n",
      "(4600, 1124.614)\n",
      "(4650, 1107.7334)\n",
      "(4700, 1049.5239)\n",
      "(4750, 1128.0171)\n",
      "(4800, 985.25647)\n",
      "(4850, 1270.4786)\n",
      "(4900, 948.2757)\n",
      "(4950, 1049.0206)\n",
      "(5000, 989.88605)\n",
      "(5050, 1102.1765)\n",
      "(5100, 1057.5452)\n",
      "(5150, 1052.1881)\n",
      "(5200, 994.93103)\n",
      "(5250, 1011.2551)\n",
      "(5300, 955.15503)\n",
      "(5350, 925.61047)\n",
      "(5400, 914.98279)\n",
      "(5450, 938.3645)\n",
      "(5500, 941.0675)\n",
      "(5550, 933.90271)\n",
      "(5600, 938.54846)\n",
      "(5650, 926.79163)\n",
      "(5700, 980.37854)\n",
      "(5750, 860.4657)\n",
      "(5800, 978.86719)\n",
      "(5850, 920.97284)\n",
      "(5900, 898.23755)\n",
      "(5950, 942.62817)\n",
      "(6000, 930.71948)\n",
      "(6050, 846.00073)\n",
      "(6100, 924.06122)\n",
      "(6150, 873.68964)\n",
      "(6200, 893.1189)\n",
      "(6250, 876.93384)\n",
      "(6300, 922.01855)\n",
      "(6350, 888.16638)\n",
      "(6400, 823.35815)\n",
      "(6450, 877.04285)\n",
      "(6500, 948.77502)\n",
      "(6550, 841.43536)\n",
      "(6600, 891.37836)\n",
      "(6650, 885.17902)\n",
      "(6700, 964.25665)\n",
      "(6750, 887.9176)\n",
      "(6800, 870.875)\n",
      "(6850, 831.31177)\n",
      "(6900, 903.45996)\n",
      "(6950, 870.23834)\n",
      "(7000, 826.61908)\n",
      "(7050, 773.34851)\n",
      "(7100, 809.46796)\n",
      "(7150, 893.95935)\n",
      "(7200, 869.6488)\n",
      "(7250, 824.95532)\n",
      "(7300, 764.65186)\n",
      "(7350, 770.16742)\n",
      "(7400, 771.62402)\n",
      "(7450, 810.41394)\n",
      "(7500, 853.25818)\n",
      "(7550, 789.45557)\n",
      "(7600, 912.47717)\n",
      "(7650, 822.05371)\n",
      "(7700, 791.79169)\n",
      "(7750, 813.5448)\n",
      "(7800, 785.49597)\n",
      "(7850, 807.19928)\n",
      "(7900, 849.76367)\n",
      "(7950, 784.97815)\n",
      "(8000, 822.13245)\n",
      "(8050, 759.09692)\n",
      "(8100, 764.57037)\n",
      "(8150, 860.27216)\n",
      "(8200, 756.69189)\n",
      "(8250, 841.29596)\n",
      "(8300, 828.66229)\n",
      "(8350, 744.76251)\n",
      "(8400, 791.12744)\n",
      "(8450, 763.0788)\n",
      "(8500, 788.56061)\n",
      "(8550, 843.93555)\n",
      "(8600, 869.28333)\n",
      "(8650, 763.93738)\n",
      "(8700, 855.82385)\n",
      "(8750, 865.21442)\n",
      "(8800, 749.97211)\n",
      "(8850, 794.83411)\n",
      "(8900, 771.83435)\n",
      "(8950, 743.15692)\n",
      "(9000, 788.92523)\n",
      "(9050, 737.3205)\n",
      "(9100, 736.80054)\n",
      "(9150, 755.7572)\n",
      "(9200, 736.1875)\n",
      "(9250, 723.45782)\n",
      "(9300, 709.62286)\n",
      "(9350, 746.41229)\n",
      "(9400, 732.14435)\n",
      "(9450, 787.59265)\n",
      "(9500, 699.35931)\n",
      "(9550, 742.52155)\n",
      "(9600, 823.77197)\n",
      "(9650, 750.20959)\n",
      "(9700, 810.16821)\n",
      "(9750, 833.1925)\n",
      "(9800, 794.00024)\n",
      "(9850, 792.09137)\n",
      "(9900, 828.81018)\n",
      "(9950, 724.46021)\n",
      "(10000, 795.9726)\n",
      "(10050, 774.53168)\n",
      "(10100, 726.33356)\n",
      "(10150, 718.69775)\n",
      "(10200, 720.36731)\n",
      "(10250, 781.99609)\n",
      "(10300, 730.4892)\n",
      "(10350, 763.57599)\n",
      "(10400, 741.56018)\n",
      "(10450, 740.33521)\n",
      "(10500, 734.32806)\n",
      "(10550, 682.91187)\n",
      "(10600, 716.98254)\n",
      "(10650, 717.04376)\n",
      "(10700, 737.25543)\n",
      "(10750, 709.62805)\n",
      "(10800, 735.95837)\n",
      "(10850, 727.85986)\n",
      "(10900, 749.2854)\n",
      "(10950, 823.46893)\n",
      "(11000, 700.8587)\n",
      "(11050, 684.94342)\n",
      "(11100, 674.15607)\n",
      "(11150, 689.84161)\n",
      "(11200, 733.68213)\n",
      "(11250, 677.2829)\n",
      "(11300, 772.1546)\n",
      "(11350, 713.73944)\n",
      "(11400, 771.74371)\n",
      "(11450, 707.59106)\n",
      "(11500, 798.27655)\n",
      "(11550, 753.82343)\n",
      "(11600, 722.82574)\n",
      "(11650, 705.71906)\n",
      "(11700, 789.07837)\n",
      "(11750, 745.13074)\n",
      "(11800, 726.0733)\n",
      "(11850, 700.57416)\n",
      "(11900, 706.84888)\n",
      "(11950, 632.71796)\n",
      "(12000, 732.6698)\n",
      "(12050, 700.19153)\n",
      "(12100, 682.12463)\n",
      "(12150, 707.65778)\n",
      "(12200, 689.8092)\n",
      "(12250, 652.3949)\n",
      "(12300, 651.3584)\n",
      "(12350, 669.7334)\n",
      "(12400, 676.03284)\n",
      "(12450, 675.19025)\n",
      "(12500, 638.88544)\n",
      "(12550, 770.77905)\n",
      "(12600, 707.64874)\n",
      "(12650, 682.32202)\n",
      "(12700, 794.33374)\n",
      "(12750, 729.26434)\n",
      "(12800, 694.79919)\n",
      "(12850, 674.39014)\n",
      "(12900, 692.14264)\n",
      "(12950, 702.40369)\n",
      "(13000, 669.29187)\n",
      "(13050, 665.89667)\n",
      "(13100, 711.01959)\n",
      "(13150, 690.05499)\n",
      "(13200, 738.25671)\n",
      "(13250, 736.1275)\n",
      "(13300, 686.05688)\n",
      "(13350, 645.90808)\n",
      "(13400, 755.54773)\n",
      "(13450, 681.78076)\n",
      "(13500, 647.11841)\n",
      "(13550, 621.63721)\n",
      "(13600, 705.47803)\n",
      "(13650, 690.63226)\n",
      "(13700, 712.40741)\n",
      "(13750, 687.659)\n",
      "(13800, 713.82812)\n",
      "(13850, 696.19489)\n",
      "(13900, 667.54852)\n",
      "(13950, 697.41412)\n",
      "(14000, 668.29767)\n",
      "(14050, 706.27094)\n",
      "(14100, 711.91455)\n",
      "(14150, 714.20703)\n",
      "(14200, 689.22253)\n",
      "(14250, 734.20471)\n",
      "(14300, 692.88556)\n",
      "(14350, 692.51355)\n",
      "(14400, 742.65881)\n",
      "(14450, 686.84479)\n",
      "(14500, 709.28192)\n",
      "(14550, 735.74261)\n",
      "(14600, 812.42303)\n",
      "(14650, 727.38977)\n",
      "(14700, 747.03339)\n",
      "(14750, 691.44165)\n",
      "(14800, 684.38599)\n",
      "(14850, 756.76221)\n",
      "(14900, 728.79382)\n",
      "(14950, 756.60596)\n",
      "(15000, 743.32916)\n",
      "(15050, 745.76202)\n",
      "(15100, 767.29565)\n",
      "(15150, 722.36993)\n",
      "(15200, 743.5296)\n",
      "(15250, 772.41492)\n",
      "(15300, 685.93512)\n",
      "(15350, 724.47778)\n",
      "(15400, 775.56287)\n",
      "(15450, 671.78296)\n",
      "(15500, 680.4881)\n",
      "(15550, 674.1781)\n",
      "(15600, 680.47974)\n",
      "(15650, 694.53247)\n",
      "(15700, 708.45148)\n",
      "(15750, 664.55371)\n",
      "(15800, 689.3504)\n",
      "(15850, 690.65747)\n",
      "(15900, 757.70264)\n",
      "(15950, 751.54584)\n",
      "(16000, 752.32562)\n",
      "(16050, 753.87329)\n",
      "(16100, 755.00903)\n",
      "(16150, 684.85736)\n",
      "(16200, 715.08588)\n",
      "(16250, 756.95953)\n",
      "(16300, 766.64648)\n",
      "(16350, 692.55585)\n",
      "(16400, 728.19366)\n",
      "(16450, 720.70374)\n",
      "(16500, 745.7207)\n",
      "(16550, 706.0426)\n",
      "(16600, 705.07758)\n",
      "(16650, 744.62305)\n",
      "(16700, 756.4646)\n",
      "(16750, 728.1582)\n",
      "(16800, 737.01123)\n",
      "(16850, 789.67883)\n",
      "(16900, 724.91968)\n",
      "(16950, 726.71851)\n",
      "(17000, 710.28082)\n",
      "(17050, 678.52704)\n",
      "(17100, 690.83868)\n",
      "(17150, 741.79572)\n",
      "(17200, 722.73529)\n",
      "(17250, 729.76874)\n",
      "(17300, 690.49756)\n",
      "(17350, 823.07886)\n",
      "(17400, 746.48535)\n",
      "(17450, 697.50586)\n",
      "(17500, 735.3551)\n",
      "(17550, 705.84235)\n",
      "(17600, 671.37146)\n",
      "(17650, 682.48859)\n",
      "(17700, 762.0979)\n",
      "(17750, 675.7196)\n",
      "(17800, 734.78217)\n",
      "(17850, 730.2821)\n",
      "(17900, 731.46814)\n",
      "(17950, 761.41205)\n",
      "(18000, 747.58954)\n",
      "(18050, 745.65906)\n",
      "(18100, 704.63403)\n",
      "(18150, 678.73492)\n",
      "(18200, 880.62286)\n",
      "(18250, 813.28796)\n",
      "(18300, 765.25745)\n",
      "(18350, 747.44061)\n",
      "(18400, 763.54169)\n",
      "(18450, 760.02509)\n",
      "(18500, 687.35229)\n",
      "(18550, 723.14722)\n",
      "(18600, 727.63928)\n",
      "(18650, 715.47186)\n",
      "(18700, 722.19299)\n",
      "(18750, 683.51068)\n",
      "(18800, 723.9187)\n",
      "(18850, 694.49396)\n",
      "(18900, 694.24274)\n",
      "(18950, 732.19495)\n",
      "(19000, 686.03381)\n",
      "(19050, 777.35254)\n",
      "(19100, 740.93457)\n",
      "(19150, 765.51123)\n",
      "(19200, 717.2229)\n",
      "(19250, 717.13202)\n",
      "(19300, 716.68365)\n",
      "(19350, 688.84161)\n",
      "(19400, 755.55792)\n",
      "(19450, 742.42163)\n",
      "(19500, 728.57098)\n",
      "(19550, 699.02179)\n",
      "(19600, 709.89062)\n",
      "(19650, 726.76727)\n",
      "(19700, 769.54187)\n",
      "(19750, 746.94061)\n",
      "(19800, 707.37585)\n",
      "(19850, 773.58972)\n",
      "(19900, 735.05896)\n",
      "(19950, 680.73737)\n",
      "test accuracy 0.9796\n"
     ]
    }
   ],
   "source": [
    "\n",
    "BNs, acc_BN, loss = [], [],[]\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(20000):\n",
    "    batch = data.train.next_batch(batch_size)\n",
    "    train_step_BN.run(feed_dict={x: batch[0], y_labels: batch[1]})\n",
    "    if i % 50 is 0:\n",
    "        res = sess.run([accuracy_BN,BN2,cross_entropy_BN],feed_dict={x: data.test.images, y_labels: data.test.labels})\n",
    "        acc_BN.append(res[0])\n",
    "        BNs.append(np.mean(res[1],axis=0)) # record the mean value of BN2 over the entire test set\n",
    "        loss.append(res[2])\n",
    "        print(i,res[2])\n",
    "print('test accuracy %g' % accuracy_BN.eval(feed_dict={\n",
    "            x: data.test.images, y_labels: data.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20167ecb10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_loss(loss):\n",
    "    x = [i for i in range(len(loss))]\n",
    "    plt.plot(x, loss)\n",
    "    plt.show()\n",
    "plot_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Weight Initialization Functions\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "#Convolutiona and Max Pooling Functions\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides = [1, 1, 1, 1], padding = 'SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./train-images-idx3-ubyte.gz\n",
      "Extracting ./train-labels-idx1-ubyte.gz\n",
      "Extracting ./t10k-images-idx3-ubyte.gz\n",
      "Extracting ./t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "data = input_data.read_data_sets(\".\",one_hot = True)\n",
    "data.train.images.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNISTdata/train-images-idx3-ubyte.gz\n",
      "Extracting MNISTdata/train-labels-idx1-ubyte.gz\n",
      "Extracting MNISTdata/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNISTdata/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist = read_data_sets(\"MNISTdata\", one_hot=True, reshape=False, validation_size=0)\n",
    "mnist.train.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNISTdata/train-images-idx3-ubyte.gz\n",
      "Extracting MNISTdata/train-labels-idx1-ubyte.gz\n",
      "Extracting MNISTdata/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNISTdata/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 256 and 784 for 'MatMul_68' (op: 'MatMul') with input shapes: [?,256], [784,10].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-8caba72b65dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0mY4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;31m#Y4 = tf.nn.dropout(Y4, pkeep)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m \u001b[0mYlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.pyc\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   1889\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m       return gen_math_ops._mat_mul(\n\u001b[0;32m-> 1891\u001b[0;31m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   1892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.pyc\u001b[0m in \u001b[0;36m_mat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   2435\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m   2436\u001b[0m         \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranspose_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranspose_b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2437\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m   2438\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2439\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2956\u001b[0m         op_def=op_def)\n\u001b[1;32m   2957\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2958\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2959\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2960\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2207\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2209\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2210\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2211\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2158\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2159\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2161\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.pyc\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, require_shape_fn)\u001b[0m\n\u001b[1;32m    625\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    626\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m                                   require_shape_fn)\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.pyc\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    689\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions must be equal, but are 256 and 784 for 'MatMul_68' (op: 'MatMul') with input shapes: [?,256], [784,10]."
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "\n",
    "\n",
    "NUM_ITERS=5000\n",
    "DISPLAY_STEP=100\n",
    "BATCH=100\n",
    "\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "# Download images and labels \n",
    "mnist = read_data_sets(\"MNISTdata\", one_hot=True, reshape=False, validation_size=0)\n",
    "\n",
    "# mnist.test (10K images+labels) -> mnist.test.images, mnist.test.labels\n",
    "# mnist.train (60K images+labels) -> mnist.train.images, mnist.test.labels\n",
    "\n",
    "# Placeholder for input images, each data sample is 28x28 grayscale images\n",
    "# All the data will be stored in X - tensor, 4 dimensional matrix\n",
    "# The first dimension (None) will index the images in the mini-batch\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "# correct answers will go here\n",
    "Y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "# Probability of keeping a node during dropout = 1.0 at test time (no dropout) and 0.75 at training time\n",
    "pkeep = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "# layers sizes\n",
    "C1 = 4  # first convolutional layer output depth\n",
    "C2 = 8  # second convolutional layer output depth\n",
    "C3 = 16 # third convolutional layer output depth\n",
    "\n",
    "FC4 = 256  # fully connected layer\n",
    "\n",
    "\n",
    "# weights - initialized with random values from normal distribution mean=0, stddev=0.1\n",
    "\n",
    "# 5x5 conv. window, 1 input channel (gray images), C1 - outputs\n",
    "W1 = tf.Variable(tf.truncated_normal([5, 5, 1, C1], stddev=0.1))\n",
    "b1 = tf.Variable(tf.truncated_normal([C1], stddev=0.1))\n",
    "# 3x3 conv. window, C1 input channels(output from previous conv. layer ), C2 - outputs\n",
    "W2 = tf.Variable(tf.truncated_normal([3, 3, C1, C2], stddev=0.1))\n",
    "b2 = tf.Variable(tf.truncated_normal([C2], stddev=0.1))\n",
    "# 3x3 conv. window, C2 input channels(output from previous conv. layer ), C3 - outputs\n",
    "W3 = tf.Variable(tf.truncated_normal([3, 3, C2, C3], stddev=0.1))\n",
    "b3 = tf.Variable(tf.truncated_normal([C3], stddev=0.1))\n",
    "# fully connected layer, we have to reshpe previous output to one dim, \n",
    "# we have two max pool operation in our network design, so our initial size 28x28 will be reduced 2*2=4\n",
    "# each max poll will reduce size by factor of 2\n",
    "W4 = tf.Variable(tf.truncated_normal([7*7*C, FC4], stddev=0.1))\n",
    "b4 = tf.Variable(tf.truncated_normal([FC4], stddev=0.1))\n",
    "\n",
    "# output softmax layer (10 digits)\n",
    "W5 = tf.Variable(tf.truncated_normal([7*7*C3, 10], stddev=0.1))\n",
    "b5 = tf.Variable(tf.truncated_normal([10], stddev=0.1))\n",
    "\n",
    "# flatten the images, unroll each image row by row, create vector[784] \n",
    "# -1 in the shape definition means compute automatically the size of this dimension\n",
    "XX = tf.reshape(X, [-1, 784])\n",
    "\n",
    "# Define the model\n",
    "\n",
    "stride = 1  # output is 28x28\n",
    "Y1 = tf.nn.relu(tf.nn.conv2d(X, W1, strides=[1, stride, stride, 1], padding='SAME') + b1)\n",
    "\n",
    "k = 2 # max pool filter size and stride, will reduce input by factor of 2\n",
    "Y2 = tf.nn.relu(tf.nn.conv2d(Y1, W2, strides=[1, stride, stride, 1], padding='SAME') + b2)\n",
    "Y2 = tf.nn.max_pool(Y2, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
    "\n",
    "#Y3 = tf.nn.relu(tf.nn.conv2d(Y2, W3, strides=[1, stride, stride, 1], padding='SAME') + b3)\n",
    "#Y3 = tf.nn.max_pool(Y3, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
    "\n",
    "# reshape the output from the third convolution for the fully connected layer\n",
    "YY = tf.reshape(Y2, shape=[-1, 7 * 7 * C3])\n",
    "\n",
    "Y4 = tf.nn.relu(tf.matmul(YY, W4) + b4)\n",
    "#Y4 = tf.nn.dropout(Y4, pkeep)\n",
    "Ylogits = tf.matmul(Y4, W5) + b5\n",
    "Y = tf.nn.softmax(Ylogits)\n",
    "\n",
    "\n",
    "\n",
    "# loss function: cross-entropy = - sum( Y_i * log(Yi) )\n",
    "# log takes the log of each element, * multiplies the tensors element by element\n",
    "# reduce_mean will add all the components in the tensor\n",
    "# so here we end up with the total cross-entropy for all images in the batch\n",
    "#cross_entropy = -tf.reduce_mean(Y_ * tf.log(Y)) * 100.0  # normalized for batches of 100 images,\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=Y_)\n",
    "cross_entropy = tf.reduce_mean(cross_entropy)*100\n",
    "\n",
    "                                                          \n",
    "# accuracy of the trained model, between 0 (worst) and 1 (best)\n",
    "correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# training, \n",
    "learning_rate = 0.003\n",
    "#train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "\n",
    "\n",
    "# matplotlib visualization\n",
    "allweights = tf.concat([tf.reshape(W1, [-1]), tf.reshape(W2, [-1]), tf.reshape(W3, [-1]), tf.reshape(W4, [-1]), tf.reshape(W5, [-1])], 0)\n",
    "allbiases  = tf.concat([tf.reshape(b1, [-1]), tf.reshape(b2, [-1]), tf.reshape(b3, [-1]), tf.reshape(b4, [-1]), tf.reshape(b5, [-1])], 0)\n",
    "\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "train_losses = list()\n",
    "train_acc = list()\n",
    "test_losses = list()\n",
    "test_acc = list()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "\n",
    "    for i in range(NUM_ITERS+1):\n",
    "        # training on batches of 100 images with 100 labels\n",
    "        batch_X, batch_Y = mnist.train.next_batch(BATCH)\n",
    "        \n",
    "        if i%DISPLAY_STEP ==0:\n",
    "            # compute training values for visualization\n",
    "            acc_trn, loss_trn, w, b = sess.run([accuracy, cross_entropy, allweights, allbiases], feed_dict={X: batch_X, Y_: batch_Y, pkeep: 1.0})\n",
    "            \n",
    "            acc_tst, loss_tst = sess.run([accuracy, cross_entropy], feed_dict={X: mnist.test.images, Y_: mnist.test.labels, pkeep: 1.0})\n",
    "            \n",
    "            print(\"#{} Trn acc={} , Trn loss={} Tst acc={} , Tst loss={}\".format(i,acc_trn,loss_trn,acc_tst,loss_tst))\n",
    "\n",
    "            train_losses.append(loss_trn)\n",
    "            train_acc.append(acc_trn)\n",
    "            test_losses.append(loss_tst)\n",
    "            test_acc.append(acc_tst)\n",
    "\n",
    "        # the back-propagation training step\n",
    "        sess.run(train_step, feed_dict={X: batch_X, Y_: batch_Y, pkeep: 0.75})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0 Trn acc=0.0799999982119 , Trn loss=230.402664185 Tst acc=0.0916000008583 , Tst loss=230.2371521\n",
      "#100 Trn acc=0.920000016689 , Trn loss=24.4795722961 Tst acc=0.919499993324 , Tst loss=27.658197403\n",
      "#200 Trn acc=0.980000019073 , Trn loss=8.4661283493 Tst acc=0.959599971771 , Tst loss=13.0228023529\n",
      "#300 Trn acc=0.97000002861 , Trn loss=15.1811494827 Tst acc=0.968200027943 , Tst loss=9.7203950882\n",
      "#400 Trn acc=0.949999988079 , Trn loss=12.807975769 Tst acc=0.963699996471 , Tst loss=11.8990278244\n",
      "#500 Trn acc=0.97000002861 , Trn loss=11.2653226852 Tst acc=0.971099972725 , Tst loss=8.52840423584\n",
      "#600 Trn acc=0.990000009537 , Trn loss=3.7403178215 Tst acc=0.977500021458 , Tst loss=7.24575901031\n",
      "#700 Trn acc=0.97000002861 , Trn loss=13.2042694092 Tst acc=0.980000019073 , Tst loss=6.5803103447\n",
      "#800 Trn acc=0.959999978542 , Trn loss=20.5152053833 Tst acc=0.949699997902 , Tst loss=15.5717372894\n",
      "#900 Trn acc=0.97000002861 , Trn loss=12.4733762741 Tst acc=0.981299996376 , Tst loss=5.64634084702\n",
      "#1000 Trn acc=0.990000009537 , Trn loss=4.5172290802 Tst acc=0.981199979782 , Tst loss=5.53609704971\n",
      "#1100 Trn acc=0.949999988079 , Trn loss=12.3956756592 Tst acc=0.980700016022 , Tst loss=5.59335803986\n",
      "#1200 Trn acc=0.980000019073 , Trn loss=7.38817882538 Tst acc=0.98400002718 , Tst loss=4.83159971237\n",
      "#1300 Trn acc=0.980000019073 , Trn loss=10.4373426437 Tst acc=0.985099971294 , Tst loss=4.55493831635\n",
      "#1400 Trn acc=0.990000009537 , Trn loss=2.61977887154 Tst acc=0.985499978065 , Tst loss=4.53087854385\n",
      "#1500 Trn acc=0.980000019073 , Trn loss=9.72544574738 Tst acc=0.985499978065 , Tst loss=4.57843017578\n",
      "#1600 Trn acc=0.990000009537 , Trn loss=4.8152217865 Tst acc=0.985199987888 , Tst loss=4.54565238953\n",
      "#1700 Trn acc=0.97000002861 , Trn loss=5.41052722931 Tst acc=0.983399987221 , Tst loss=4.56794309616\n",
      "#1800 Trn acc=1.0 , Trn loss=1.99879813194 Tst acc=0.985400021076 , Tst loss=4.5631775856\n",
      "#1900 Trn acc=1.0 , Trn loss=1.18597269058 Tst acc=0.985000014305 , Tst loss=4.35480213165\n",
      "#2000 Trn acc=0.980000019073 , Trn loss=5.1121468544 Tst acc=0.981999993324 , Tst loss=5.6780333519\n",
      "#2100 Trn acc=0.97000002861 , Trn loss=6.90740251541 Tst acc=0.987200021744 , Tst loss=3.98838734627\n",
      "#2200 Trn acc=0.990000009537 , Trn loss=2.6662106514 Tst acc=0.984600007534 , Tst loss=4.36311149597\n",
      "#2300 Trn acc=0.990000009537 , Trn loss=4.70902490616 Tst acc=0.981000006199 , Tst loss=5.66674041748\n",
      "#2400 Trn acc=0.980000019073 , Trn loss=3.32885336876 Tst acc=0.984899997711 , Tst loss=4.29006195068\n",
      "#2500 Trn acc=1.0 , Trn loss=0.274223089218 Tst acc=0.988499999046 , Tst loss=3.95835757256\n",
      "#2600 Trn acc=1.0 , Trn loss=0.210991412401 Tst acc=0.986000001431 , Tst loss=4.35131502151\n",
      "#2700 Trn acc=0.980000019073 , Trn loss=3.2037935257 Tst acc=0.985899984837 , Tst loss=4.2416882515\n",
      "#2800 Trn acc=1.0 , Trn loss=1.35858678818 Tst acc=0.98710000515 , Tst loss=3.66024851799\n",
      "#2900 Trn acc=0.980000019073 , Trn loss=7.57226848602 Tst acc=0.989499986172 , Tst loss=3.54613852501\n",
      "#3000 Trn acc=0.990000009537 , Trn loss=3.87222313881 Tst acc=0.985199987888 , Tst loss=4.24001312256\n",
      "#3100 Trn acc=0.980000019073 , Trn loss=4.2907242775 Tst acc=0.988399982452 , Tst loss=3.78056693077\n",
      "#3200 Trn acc=0.990000009537 , Trn loss=1.32470929623 Tst acc=0.988200008869 , Tst loss=3.93938016891\n",
      "#3300 Trn acc=0.990000009537 , Trn loss=5.54379796982 Tst acc=0.98710000515 , Tst loss=3.61947011948\n",
      "#3400 Trn acc=1.0 , Trn loss=1.61025857925 Tst acc=0.987500011921 , Tst loss=4.26286363602\n",
      "#3500 Trn acc=1.0 , Trn loss=0.408618628979 Tst acc=0.99019998312 , Tst loss=3.05489683151\n",
      "#3600 Trn acc=0.990000009537 , Trn loss=3.95876026154 Tst acc=0.987900018692 , Tst loss=3.84305858612\n",
      "#3700 Trn acc=1.0 , Trn loss=1.01425719261 Tst acc=0.98860001564 , Tst loss=3.51154613495\n",
      "#3800 Trn acc=0.990000009537 , Trn loss=1.74378585815 Tst acc=0.986999988556 , Tst loss=4.0606637001\n",
      "#3900 Trn acc=0.990000009537 , Trn loss=3.73466300964 Tst acc=0.987999975681 , Tst loss=3.80024695396\n",
      "#4000 Trn acc=1.0 , Trn loss=0.709104418755 Tst acc=0.987200021744 , Tst loss=3.93149662018\n",
      "#4100 Trn acc=1.0 , Trn loss=0.215936779976 Tst acc=0.986000001431 , Tst loss=4.88315868378\n",
      "#4200 Trn acc=0.990000009537 , Trn loss=1.27836418152 Tst acc=0.986199975014 , Tst loss=4.45223426819\n",
      "#4300 Trn acc=0.980000019073 , Trn loss=2.65676307678 Tst acc=0.989499986172 , Tst loss=3.29997420311\n",
      "#4400 Trn acc=0.990000009537 , Trn loss=1.43275916576 Tst acc=0.987200021744 , Tst loss=4.04277849197\n",
      "#4500 Trn acc=0.990000009537 , Trn loss=2.97069907188 Tst acc=0.987699985504 , Tst loss=3.79852437973\n",
      "#4600 Trn acc=1.0 , Trn loss=0.217614144087 Tst acc=0.984200000763 , Tst loss=5.43863010406\n",
      "#4700 Trn acc=0.990000009537 , Trn loss=5.02492237091 Tst acc=0.988200008869 , Tst loss=3.80401325226\n",
      "#4800 Trn acc=0.990000009537 , Trn loss=1.72564840317 Tst acc=0.987299978733 , Tst loss=4.02934455872\n",
      "#4900 Trn acc=0.990000009537 , Trn loss=1.89633262157 Tst acc=0.988499999046 , Tst loss=3.68374586105\n",
      "#5000 Trn acc=0.990000009537 , Trn loss=1.96181726456 Tst acc=0.986999988556 , Tst loss=4.75748682022\n",
      "test accuracy 0.9879\n"
     ]
    }
   ],
   "source": [
    "#import visualizations as vis\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "\n",
    "epsilon = 1e-3\n",
    "NUM_ITERS=5000\n",
    "DISPLAY_STEP=100\n",
    "BATCH=100\n",
    "\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "# mnist.test (10K images+labels) -> mnist.test.images, mnist.test.labels\n",
    "# mnist.train (60K images+labels) -> mnist.train.images, mnist.test.labels\n",
    "\n",
    "# Placeholder for input images, each data sample is 28x28 grayscale images\n",
    "# All the data will be stored in X - tensor, 4 dimensional matrix\n",
    "# The first dimension (None) will index the images in the mini-batch\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "# correct answers will go here\n",
    "Y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "# Probability of keeping a node during dropout = 1.0 at test time (no dropout) and 0.75 at training time\n",
    "pkeep = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "# layers sizes\n",
    "C1,C2,C3 = 4,8,16 \n",
    "FC4 = 256  # fully connected layer\n",
    "\n",
    "\n",
    "# weights - initialized with random values from normal distribution mean=0, stddev=0.1\n",
    "\n",
    "# 5x5 conv. window, 1 input channel (gray images), C1 - outputs\n",
    "W1 = get_weight([5, 5, 1, C1])\n",
    "b1 = get_bias([C1])\n",
    "# 3x3 conv. window, C1 input channels(output from previous conv. layer ), C2 - outputs\n",
    "W2 = get_weight([3, 3, C1, C2])\n",
    "b2 = get_bias([C2])\n",
    "\n",
    "# 3x3 conv. window, C2 input channels(output from previous conv. layer ), C3 - outputs\n",
    "W3 = get_weight([3, 3, C2, C3])\n",
    "b3 = get_bias([C3])\n",
    "\n",
    "# fully connected layer, we have to reshpe previous output to one dim, \n",
    "# we have two max pool operation in our network design, so our initial size 28x28 will be reduced 2*2=4\n",
    "# each max poll will reduce size by factor of 2\n",
    "W4 = get_weight([7*7*C3, FC4])\n",
    "b4= get_bias([FC4])\n",
    "\n",
    "\n",
    "# output softmax layer (10 digits)\n",
    "W5 = get_weight([FC4, 10])\n",
    "b5= get_bias([10])\n",
    "\n",
    "\n",
    "# flatten the images, unroll each image row by row, create vector[784] \n",
    "# -1 in the shape definition means compute automatically the size of this dimension\n",
    "XX = tf.reshape(X, [-1, 784])\n",
    "\n",
    "# Define the model\n",
    "\n",
    "Y1C = conv2d(X, W1) + b1\n",
    "batch_mean1, batch_var1 = tf.nn.moments(Y1C,[0])\n",
    "scale1 = tf.Variable(tf.ones([C1]))\n",
    "beta1 = tf.Variable(tf.zeros([C1]))\n",
    "BN1 = tf.nn.batch_normalization(Y1C,batch_mean1,batch_var1,beta1,scale1,epsilon)\n",
    "BN1 = tf.nn.relu(BN1)\n",
    "\n",
    "\n",
    "Y2C = conv2d(BN1, W2) + b2\n",
    "batch_mean2, batch_var2 = tf.nn.moments(Y2C,[0])\n",
    "scale2 = tf.Variable(tf.ones([C2]))\n",
    "beta2 = tf.Variable(tf.zeros([C2]))\n",
    "BN2 = tf.nn.batch_normalization(Y2C,batch_mean2,batch_var2,beta2,scale2,epsilon)\n",
    "BN2 = tf.nn.relu(BN2)\n",
    "\n",
    "Y2 = max_pool_2x2(BN2)\n",
    "#Y2 = tf.nn.max_pool(Y2, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
    "\n",
    "Y3 = tf.nn.relu(conv2d(Y2, W3) + b3)\n",
    "Y3C = conv2d(Y2, W3) + b3\n",
    "batch_mean3, batch_var3 = tf.nn.moments(Y3C,[0])\n",
    "scale3 = tf.Variable(tf.ones([C3]))\n",
    "beta3 = tf.Variable(tf.zeros([C3]))\n",
    "BN3 = tf.nn.batch_normalization(Y3C,batch_mean3,batch_var3,beta3,scale3,epsilon)\n",
    "BN3 = tf.nn.relu(BN3)\n",
    "\n",
    "\n",
    "\n",
    "Y3 = max_pool_2x2(BN3)\n",
    "#Y3 = tf.nn.max_pool(Y3, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
    "\n",
    "# reshape the output from the third convolution for the fully connected layer\n",
    "YY = tf.reshape(Y3, shape=[-1, 7 * 7 * C3])\n",
    "\n",
    "Y4 = tf.nn.relu(tf.matmul(YY, W4) + b4)\n",
    "#Y4 = tf.nn.dropout(Y4, pkeep)\n",
    "Ylogits = tf.matmul(Y4, W5) + b5\n",
    "Y = tf.nn.softmax(Ylogits)\n",
    "\n",
    "\n",
    "\n",
    "# loss function: cross-entropy = - sum( Y_i * log(Yi) )\n",
    "# log takes the log of each element, * multiplies the tensors element by element\n",
    "# reduce_mean will add all the components in the tensor\n",
    "# so here we end up with the total cross-entropy for all images in the batch\n",
    "#cross_entropy = -tf.reduce_mean(Y_ * tf.log(Y)) * 100.0  # normalized for batches of 100 images,\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=Y_)\n",
    "cross_entropy = tf.reduce_mean(cross_entropy)*100\n",
    "\n",
    "                                                          \n",
    "# accuracy of the trained model, between 0 (worst) and 1 (best)\n",
    "correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# training, \n",
    "learning_rate = 0.003\n",
    "#train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "\n",
    "\n",
    "# matplotlib visualization\n",
    "allweights = tf.concat([tf.reshape(W1, [-1]), tf.reshape(W2, [-1]), tf.reshape(W3, [-1]), tf.reshape(W4, [-1]), tf.reshape(W5, [-1])], 0)\n",
    "allbiases  = tf.concat([tf.reshape(b1, [-1]), tf.reshape(b2, [-1]), tf.reshape(b3, [-1]), tf.reshape(b4, [-1]), tf.reshape(b5, [-1])], 0)\n",
    "\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "train_losses_batch = list()\n",
    "train_acc_batch = list()\n",
    "test_losses_batch = list()\n",
    "test_acc_batch = list()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "\n",
    "    for i in range(NUM_ITERS+1):\n",
    "        # training on batches of 100 images with 100 labels\n",
    "        batch_X, batch_Y = mnist.train.next_batch(BATCH)\n",
    "        \n",
    "        if i%DISPLAY_STEP ==0:\n",
    "            # compute training values for visualization\n",
    "            acc_trn, loss_trn, w, b = sess.run([accuracy, cross_entropy, allweights, allbiases], feed_dict={X: batch_X, Y_: batch_Y, pkeep: 1.0})\n",
    "            \n",
    "            acc_tst, loss_tst = sess.run([accuracy, cross_entropy], feed_dict={X: mnist.test.images, Y_: mnist.test.labels, pkeep: 1.0})\n",
    "            \n",
    "            print(\"#{} Trn acc={} , Trn loss={} Tst acc={} , Tst loss={}\".format(i,acc_trn,loss_trn,acc_tst,loss_tst))\n",
    "\n",
    "            train_losses_batch.append(loss_trn)\n",
    "            train_acc_batch.append(acc_trn)\n",
    "            test_losses_batch.append(loss_tst)\n",
    "            test_acc_batch.append(acc_tst)\n",
    "\n",
    "        # the back-propagation training step\n",
    "        sess.run(train_step, feed_dict={X: batch_X, Y_: batch_Y, pkeep: 0.75})\n",
    "    print('test accuracy %g' % accuracy.eval(feed_dict={\n",
    "            X: mnist.test.images, Y_: mnist.test.labels, pkeep: 1.0}))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkVPQi4hETkEvIhK5/w85C+Hk86ByyAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1c7f5e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss(loss,loss1):\n",
    "    x = [i for i in range(len(loss))]\n",
    "    plt.plot(x, loss)\n",
    "    plt.plot(x, loss1)\n",
    "    plt.show()\n",
    "plot_loss(test_losses,test_losses_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch = []\n",
    "no_batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'Placeholder_115' with dtype float and shape [?,10]\n\t [[Node: Placeholder_115 = Placeholder[dtype=DT_FLOAT, shape=[?,10], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op u'Placeholder_115', defined at:\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-59-d1ab25e4f925>\", line 31, in <module>\n    labels = tf.placeholder(tf.float32, [None, 10])\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 1599, in placeholder\n    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 3091, in _placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder_115' with dtype float and shape [?,10]\n\t [[Node: Placeholder_115 = Placeholder[dtype=DT_FLOAT, shape=[?,10], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-a3c1583d418d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mbatch_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_opt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmerged\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_ys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder_115' with dtype float and shape [?,10]\n\t [[Node: Placeholder_115 = Placeholder[dtype=DT_FLOAT, shape=[?,10], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op u'Placeholder_115', defined at:\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-59-d1ab25e4f925>\", line 31, in <module>\n    labels = tf.placeholder(tf.float32, [None, 10])\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 1599, in placeholder\n    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 3091, in _placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/Users/chunyilyu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder_115' with dtype float and shape [?,10]\n\t [[Node: Placeholder_115 = Placeholder[dtype=DT_FLOAT, shape=[?,10], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True, reshape=False)\n",
    "\n",
    "def fully_connected(prev_layer, num_units, batch_norm, is_training=False):\n",
    "    layer = tf.layers.dense(prev_layer, num_units, use_bias=False, activation=None)\n",
    "    if batch_norm:\n",
    "    \tlayer = tf.layers.batch_normalization(layer, training=is_training)\n",
    "    layer = tf.nn.relu(layer)\n",
    "    return layer\n",
    "\n",
    "def conv_layer(prev_layer, layer_depth, batch_norm, is_training=False):\n",
    "    strides = 2 if layer_depth % 3 == 0 else 1\n",
    "    conv_layer = tf.layers.conv2d(prev_layer, layer_depth*4, 3, strides, 'same', use_bias=False, activation=None)\n",
    "    if batch_norm:\n",
    "    \tconv_layer = tf.layers.batch_normalization(conv_layer, training=is_training)\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    return conv_layer\n",
    "\n",
    "\n",
    "num_batches = 6000\n",
    "batch_size = 110\n",
    "learning_rate = 0.002\n",
    "\n",
    "layer_num = 7\n",
    "\n",
    "for learning_rate in [0.1]:\n",
    "    for batch_norm in [False]:\n",
    "\n",
    "        inputs = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "        labels = tf.placeholder(tf.float32, [None, 10])\n",
    "        is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "        #batch_norm = False\n",
    "\n",
    "        layer = inputs\n",
    "        for layer_i in range(1, 1+layer_num):\n",
    "            layer = conv_layer(layer, layer_i, batch_norm, is_training)\n",
    "\n",
    "\n",
    "        orig_shape = layer.get_shape().as_list()\n",
    "        layer = tf.reshape(layer, shape=[-1, orig_shape[1] * orig_shape[2] * orig_shape[3]])\n",
    "\n",
    "        layer = fully_connected(layer, 100, batch_norm, is_training)\n",
    "\n",
    "        logits = tf.layers.dense(layer, 10)\n",
    "            \n",
    "        model_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "        tf.summary.scalar('conv_loss',model_loss)\n",
    "\n",
    "        if batch_norm:  \n",
    "            with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "                train_opt = tf.train.GradientDescentOptimizer(learning_rate).minimize(model_loss)\n",
    "        else:\n",
    "            train_opt = tf.train.GradientDescentOptimizer(learning_rate).minimize(model_loss)\n",
    "\n",
    "        correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(labels,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            \n",
    "\n",
    "        #with tf.Session() as sess:\n",
    "        sess = tf.Session()\n",
    "\n",
    "        merged = tf.summary.merge_all()\n",
    "        if batch_norm: \n",
    "            logdir = \"mnist/conv/SGD_batchnorm_\"+str(learning_rate)\n",
    "        else:\n",
    "            logdir = \"mnist/conv/SGD_no_batchnorm\"+str(learning_rate)\n",
    "\n",
    "        writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for batch_i in range(num_batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            _,summary = sess.run([train_opt,merged], {inputs: batch_xs, labels: batch_ys, is_training: True})\n",
    "            \n",
    "            writer.add_summary(summary, batch_i)\n",
    "\n",
    "            if batch_i % 200 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: mnist.validation.images,labels: mnist.validation.labels,is_training: False})\n",
    "                print('Batch: {:>2}: Validation loss: {:>3.5f}, Validation accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "            elif batch_i % 50 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: batch_xs, labels: batch_ys, is_training: False})\n",
    "                print('Batch: {:>2}: Training loss: {:>3.5f}, Training accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "\n",
    "                # At the end, score the final accuracy for both the validation and test sets\n",
    "        acc = sess.run(accuracy, {inputs: mnist.validation.images,labels: mnist.validation.labels,is_training: False})\n",
    "        print('Final validation accuracy: {:>3.5f}'.format(acc))\n",
    "        acc = sess.run(accuracy, {inputs: mnist.test.images,labels: mnist.test.labels,is_training: False})\n",
    "        print('Final test accuracy: {:>3.5f}'.format(acc))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
